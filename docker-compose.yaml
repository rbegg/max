    # master-project/docker-compose.yaml

    services:
      # Speech-to-Text Application
      stt:
        build:
          context: ./services/max-stt  # Points to the app's directory
          args:
            - PYTHON_VERSION=3.11
        #container_name: max-stt
        # image: image name set in override files
        # No ports exposed here in production!
        environment:
          # ... environment variables ...
          - APP_NAME=stt
          - MODEL_SIZE=${STT_MODEL_SIZE:-large-v3}
          - DEVICE=${STT_DEVICE:-cuda}
          - COMPUTE_TYPE=${STT_COMPUTE_TYPE:-float16}
        volumes:
          - model_cache:/home/appuser/.cache
          # ... other volumes ...
          # --- GPU / CPU Configuration ---
          # To switch to CPU, comment out the 'deploy' section below and
          # set DEVICE=cpu and COMPUTE_TYPE=int8 in the environment variables.
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]

      # LangChain Application
#      max:
#        build:
#          context: ./services/max
        # ...

      # Ollama Service
      # only available to backend services, no host mapping

      ollama:
        build: ./services/ollama
        image: ollama-gpu
        restart: unless-stopped

        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]

        volumes:
          # Mount the named volume to the NEW non-root user's models directory
          - ollama_models:/home/ollama/.ollama

      # Reverse Proxy (this is the public gateway)
      proxy:
        image: nginx:latest
        # Port mapping and volumes will be defined in dev/prod override files

    volumes:
      model_cache:
        external: true
      ollama_models:
        external: true
        # ...
