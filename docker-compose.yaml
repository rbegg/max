    # master-project/docker-compose.yaml

    services:
      # -------------------------------------------------------------------------------------
      # web-server SERVICE
      # -------------------------------------------------------------------------------------
      web-server:
        build:
          context: ./services/web-server
          dockerfile: Dockerfile
          # Target is set in override files
          args:
            - PYTHON_VERSION=3.11
        expose:
          - "80"
        user: appuser

      # -------------------------------------------------------------------------------------
      # STT SERVICE
      # -------------------------------------------------------------------------------------
      stt:
        build:
          context: ./services/max-stt  # Points to the app's directory
          dockerfile: Dockerfile
          # Target is set in override files
          args:
            - PYTHON_VERSION=3.11
        #container_name: max-stt
        # image: image name set in override files
        # No ports exposed here in production!
        environment:
          # ... environment variables ...
          - APP_NAME=stt
          - MODEL_SIZE=${STT_MODEL_SIZE:-large-v3}
          - DEVICE=${STT_DEVICE:-cuda}
          - COMPUTE_TYPE=${STT_COMPUTE_TYPE:-float16}
        user: appuser
        volumes:
          - model_cache:/home/appuser/.cache
          # ... other volumes ...
          # --- GPU / CPU Configuration ---
          # To switch to CPU, comment out the 'deploy' section below and
          # set DEVICE=cpu and COMPUTE_TYPE=int8 in the environment variables.
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]

      # -------------------------------------------------------------------------------------
      # ASSISTANT SERVICE
      # -------------------------------------------------------------------------------------
      assistant:
        build:
          context: ./services/max-assistant
          dockerfile: Dockerfile
          # Target is set in override files
          args:
            - PYTHON_VERSION=3.11
        environment:
          - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
          - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-llama3}
        user: appuser
        depends_on:
          - stt


      # -------------------------------------------------------------------------------------
      # Ollama SERVICE
      # -------------------------------------------------------------------------------------
      ollama:
        build:
          context: ./services/ollama
          dockerfile: Dockerfile
        image: ollama-gpu
        restart: unless-stopped
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]
        volumes:
          # Mount the named volume to the NEW non-root user's models directory
          - ollama_models:/home/ollama/.ollama

      # -------------------------------------------------------------------------------------
      # proxy SERVICE
      # -------------------------------------------------------------------------------------
      proxy:
        image: nginx:latest
        # Port mapping and volumes will be defined in dev/prod override files
        depends_on:
          - stt
          - assistant
          - web-server
        environment:
          # Define the variables used in the proxy/nginx/*.conf.template files to gen nginx config
          # values read from the .env file will override the defaults here
          - SERVER_NAME=${SERVER_NAME} # <-- IMPORTANT: Change this in your .env file
          - PROXY_HTTP_PORT=${PROXY_HTTP_PORT:-80}
          - PROXY_HTTPS_PORT=${PROXY_HTTPS_PORT:-443}
          - PROXY_WORK_PROCESSES=${PROXY_WORK_PROCESSES:-auto}
          - ASSISTANT_HOST=${ASSISTANT_HOST:-assistant}
          - SSL_CERTIFICATE_PATH= ${SSL_CERTIFICATE_PATH}
          - SSL_CERTIFICATE_KEY_PATH= ${SSL_CERTIFICATE_KEY_PATH}

    volumes:
      model_cache:
        external: true
      ollama_models:
        external: true
        # ...
