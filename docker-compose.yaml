    # master-project/docker-compose.yaml

    services:
      # Speech-to-Text Application
      stt:
        build:
          context: ./services/max-stt  # Points to the app's directory
          args:
            - PYTHON_VERSION=3.11
        #container_name: max-stt
        # image: image name set in override files
        # No ports exposed here in production!
        environment:
          # ... environment variables ...
          - APP_NAME=stt
          - MODEL_SIZE=${STT_MODEL_SIZE:-large-v3}
          - DEVICE=${STT_DEVICE:-cuda}
          - COMPUTE_TYPE=${STT_COMPUTE_TYPE:-float16}
        volumes:
          - model_cache:/home/appuser/.cache
          # ... other volumes ...
          # --- GPU / CPU Configuration ---
          # To switch to CPU, comment out the 'deploy' section below and
          # set DEVICE=cpu and COMPUTE_TYPE=int8 in the environment variables.
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]

      # max-assistant Service
      assistant:
        build:
          context: ./services/max-assistant
        depends_on:
          - stt
        # ...

      web-server:
        build: ./services/web-server
        # No ports are published; access is only via Nginx

      # Ollama Service
      # only available to backend services, no host mapping

      ollama:
        build: ./services/ollama
        image: ollama-gpu
        restart: unless-stopped

        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]

        volumes:
          # Mount the named volume to the NEW non-root user's models directory
          - ollama_models:/home/ollama/.ollama

      # Reverse Proxy (this is the public gateway)
      proxy:
        image: nginx:latest
        # Port mapping and volumes will be defined in dev/prod override files
        depends_on:
          - stt
          - assistant
          - web-server
        environment:
          # Define the variables used in the template
          - SERVER_NAME=${SERVER_NAME} # <-- IMPORTANT: Change this in your .env file
          - PROXY_WORK_PROCESSES=${PROXY_WORK_PROCESSES:-auto}
          #- ASSISTANT_HOST=${ASSISTANT_HOST:-assistant}
          - ASSISTANT_HOST=${ASSISTANT_HOST:-stt}
          - SSL_CERTIFICATE_PATH= ${SSL_CERTIFICATE_PATH}
          - SSL_CERTIFICATE_KEY_PATH= ${SSL_CERTIFICATE_KEY_PATH}

    volumes:
      model_cache:
        external: true
      ollama_models:
        external: true
        # ...
