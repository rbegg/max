    # master-project/docker-compose.yaml

    services:
      # -------------------------------------------------------------------------------------
      # web-server SERVICE
      # -------------------------------------------------------------------------------------
      web-server:
        build:
          context: ./services/web-server
          dockerfile: Dockerfile
          # Target is set in override files
          args:
            - PYTHON_VERSION=3.11
        expose:
          - "80"
        user: appuser
        healthcheck:
          # This command now calls your new API endpoint
          test: [ "CMD", "curl", "-f", "http://localhost:80/health" ]
          interval: 60s
          timeout: 15s
          retries: 3
          start_period: 60s
        depends_on:
          - assistant

      # -------------------------------------------------------------------------------------
      # STT SERVICE - Speach to text
      # -------------------------------------------------------------------------------------
      stt:
        build:
          context: ./services/max-stt  # Points to the app's directory
          dockerfile: Dockerfile
          # Target is set in override files
          args:
            - PYTHON_VERSION=3.11
        #container_name: max-stt
        # image: image name set in override files
        # No ports exposed here in production!
        environment:
          # ... environment variables ...
          - APP_NAME=stt
          - MODEL_SIZE=${STT_MODEL_SIZE:-large-v3}
          - DEVICE=${STT_DEVICE:-cuda}
          - COMPUTE_TYPE=${STT_COMPUTE_TYPE:-float16}
        user: appuser
        volumes:
          - model_cache:/home/appuser/.cache
          # ... other volumes ...
          # --- GPU / CPU Configuration ---
          # To switch to CPU, comment out the 'deploy' section below and
          # set DEVICE=cpu and COMPUTE_TYPE=int8 in the environment variables.
        healthcheck:
          # This command now calls your new API endpoint
          test: [ "CMD", "curl", "-f", "http://localhost:80/health" ]
          interval: 60s
          timeout: 15s
          retries: 3
          start_period: 60s
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]

      # -------------------------------------------------------------------------------------
      # TTS SERVICE - Text To Speach
      # -------------------------------------------------------------------------------------
      tts:
        build:
          context: ./services/max-tts  # Points to the app's directory
          dockerfile: Dockerfile
        command: --voice ${TTS_VOICE:-en_US-lessac-medium}
        volumes:
          - stt_models:/config
        expose:
          - 10200
        healthcheck:
          test: [ "CMD", "nc", "-z", "localhost", "10200" ]
          interval: 60s
          timeout: 10s
          retries: 3
          start_period: 60s


      # -------------------------------------------------------------------------------------
      # ASSISTANT SERVICE
      # -------------------------------------------------------------------------------------
      assistant:
        build:
          context: ./services/max-assistant
          dockerfile: Dockerfile
          # Target is set in override files
          args:
            - PYTHON_VERSION=3.11
        environment:
          - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
          - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-llama3}
          - TTS_VOICE=${TTS_VOICE:-en_US-lessac-medium}
          - LOG_LEVEL=${ASSISTANT_LOG_LEVEL:-INFO}
          - NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
          - NEO4J_USER=${NEO4J_USER}
          - NEO4J_PASSWORD=${NEO4J_PASSWORD}
        user: appuser
        healthcheck:
          # This command now calls your new API endpoint
          test: [ "CMD", "curl", "-f", "http://localhost:80/health" ]
          interval: 60s
          timeout: 15s
          retries: 3
          start_period: 60s
        depends_on:
          - stt
          - tts
          - ollama
        volumes:
          - "/etc/timezone:/etc/timezone:ro"
          - "/etc/localtime:/etc/localtime:ro"


      # -------------------------------------------------------------------------------------
      # Ollama SERVICE
      # -------------------------------------------------------------------------------------
      ollama:
        build:
          context: ./services/ollama
          dockerfile: Dockerfile
        image: ollama-gpu
        restart: unless-stopped
        environment:
          - OLLAMA_KEEP_ALIVE=-1
          - DEBUG=${OLLAMA_DEBUG:-0}
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]
        volumes:
          # Mount the named volume to the NEW non-root user's models directory
          - ollama_models:/home/ollama/.ollama
        healthcheck:
          # Check if the Ollama API is responding.
          test: [ "CMD", "curl", "-f", "http://localhost:11434" ]
          interval: 120s
          timeout: 15s
          retries: 3
          start_period: 240s

      # -------------------------------------------------------------------------------------
      # proxy SERVICE
      # -------------------------------------------------------------------------------------
      proxy:
        image: nginx:latest
        # Port mapping and volumes will be defined in dev/prod override files
        depends_on:
          - stt
          - assistant
          - web-server
        environment:
          # Define the variables used in the proxy/nginx/*.conf.template files to gen nginx config
          # values read from the .env file will override the defaults here
          - SERVER_NAME=${SERVER_NAME} # <-- IMPORTANT: Change this in your .env file
          - PROXY_HTTP_PORT=${PROXY_HTTP_PORT:-80}
          - PROXY_HTTPS_PORT=${PROXY_HTTPS_PORT:-443}
          - PROXY_WORK_PROCESSES=${PROXY_WORK_PROCESSES:-auto}
          - ASSISTANT_HOST=${ASSISTANT_HOST:-assistant}
          - SSL_CERTIFICATE_PATH= ${SSL_CERTIFICATE_PATH}
          - SSL_CERTIFICATE_KEY_PATH= ${SSL_CERTIFICATE_KEY_PATH}
        healthcheck:
          # This command checks if Nginx is responding with a success code
          test: [ "CMD", "curl", "-f", "http://localhost" ]
          interval: 60s
          timeout: 10s
          retries: 3
          start_period: 60s

      # -------------------------------------------------------------------------------------
      # node4j Database SERVICE
      # -------------------------------------------------------------------------------------
      neo4j:
        image: neo4j:latest
        container_name: neo4j_db
        expose:
          - "7474" # Neo4j Browser
          - "76877" # Bolt (database connection)
        environment:
          - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD} # Set username/password
          - NEO4J_PLUGINS=["apoc"] # Optional: include useful plugins
        volumes:
          - neo4j_data:/data # Persist database data
        healthcheck:
          test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1" ]
          interval: 5s
          timeout: 10s
          retries: 3
          start_period: 10s

    volumes:
      model_cache:
        external: true
      ollama_models:
        external: true
      stt_models:
        external: true
      neo4j_data:
        external: true